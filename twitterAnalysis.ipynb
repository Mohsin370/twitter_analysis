{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Slappy is holding a steady floor at 100k mcap ...\n",
       "1     400 million,  you done fucked up, shoulda divo...\n",
       "2      Ouais je vais courir jusqu'√† Marseille pour a...\n",
       "3         : Wow.This really happened.üò≥ (He deserved it)\n",
       "4      What Will Smith said....\\nHaileeSteinfeld Wil...\n",
       "5      Where‚Äôs WillSmith when you need him!??? Welp ...\n",
       "6     The Will Smith slap will never go away. YouTub...\n",
       "7                                              _ajid   \n",
       "8     „Ç¶„Ç£„É´„Éª„Çπ„Éü„Çπ„ÅåË∂Ö‰∏ÄÊµÅÊ¨∫Â∏´ÂΩπ„Åß‰∏ªÊºî„ÄÅÂÖ±Êºî„ÅØ„Éû„Éº„Ç¥„ÉÉ„Éà„Éª„É≠„Éì„ÉºÔºèÊò†Áîª„Äé„Éï„Ç©„Éº„Ç´„Çπ„Äè‰∫àÂëäÁ∑® \\n...\n",
       "9     : \"Desde LaPalma hasta Ucrania\":\\n\\n\\n\\nCOVID1...\n",
       "10       Klo Willsmith masih satu keluarga sama Hb B...\n",
       "11                          Eat a hot bowl a dick nigga\n",
       "12    :   Shockingly ugly . Your tears mean nothing,...\n",
       "13                                 willsmith chrisrock \n",
       "14                    : WillSmith\\n\\nCan't slap us all \n",
       "15    See yall hooked on diss tracks and who's hot b...\n",
       "16    _Official: Chinese one-sidedly support Will Sm...\n",
       "17    : If Hollywood is going to go to such great le...\n",
       "18    _Official: Chinese one-sidedly support Will Sm...\n",
       "19       I think you guys don't understand true love...\n",
       "Name: full_text, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import re\n",
    "# from textblob import TextBlob\n",
    "# from wordcloud import WordCloud\n",
    "# df = pd.read_json('output.json')\n",
    "# df.to_csv('data.csv')\n",
    "consumer_key = 'E4la57r5OwcofajnrdtKJTE1W'\n",
    "consumer_secret = '7m9mzTwMMSEVVmEHtzK7suTUgwyEEJYdLckNoFC7husEEgTao9'\n",
    "access_token = '2942662970-tUvaWqtMJCSZcXtBUXUkBIjQNsXmDV0Fv9REkQ8'\n",
    "access_token_secret = 't0zfRbilLcGWDKMsOExi1MSR4AUFbA1WBRcZTL3U5xnVK'\n",
    "    # twitter authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# df = pd.read_csv('data.csv') #Reading CSV file\n",
    "tweetsDF = pd.read_json('output.json') #Reading json file\n",
    "# df = pd.DataFrame([tweet.full_text for tweet in tweets], columns=['Tweets'])\n",
    "# print (tweetsDF['full_text'])\n",
    "# for index, row in tweetsDF[0:10].iterrows():\n",
    "#     print(index,row['full_text'])\n",
    "\n",
    "\n",
    "\n",
    "def cleanData(tweet):\n",
    "    tweet = re.sub(r'@[A-Za-z0-9]+','', tweet) #remove @mentions\n",
    "    tweet = re.sub(r'#+','', tweet) #remove # symbols\n",
    "    tweet = re.sub(r'RT[\\s]+','', tweet) #remove RT\n",
    "    tweet = re.sub(r'https?:\\/\\/\\S+','', tweet) #remove hyperlink\n",
    "    return tweet\n",
    "\n",
    "\n",
    "\n",
    "# apply regex on tweet to clean data\n",
    "tweetsDF['full_text'] = tweetsDF['full_text'].apply(cleanData)\n",
    "tweetsDF['full_text'][0:20]\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "259924ecf78bfd522deae4439517e0cf1c0cf3195f2108ca9ff9ba25f7dc04f6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
